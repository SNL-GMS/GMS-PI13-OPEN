# =============================================================================
# GMS System Configuration Properties
# =============================================================================
# Each system instance will have its own version of etcd with these 
# properties.
#
# Key names may be prefixed with a control name and a dot ("."). The
# more specific value with the control name will override the more
# general value if present.
# =============================================================================

# Values common to all GMS controls
port = 8080
idle-timeout = PT30S
min-threads = 10
max-threads = 100

# Default DB connection info
sql_url = jdbc:postgresql://postgresql-gms:5432/gms?reWriteBatchedInserts=true
sql_user = gms_soh_application
sql_password = gmsdb:postgres:gms_soh_application:smidgeons-offerers-reducers
c3p0_connection_pool_size = 10

waveforms-coi.host=osd-waveforms-repository-service
signal-detection-coi.host=osd-signaldetection-repository-service
station-reference-coi.host=osd-stationreference-coi-service
performance-monitoring-coi.host=osd-performance-monitoring-service

osd.host=frameworks-osd-service
osd.c3p0_connection_pool_size = 100

# Values common to all GMS Services
experimental-enabled = false

osd-rsdf-kafka-consumer.host = frameworks-osd-rsdf-kafka-consumer
osd-rsdf-kafka-consumer.application-id = frameworks-osd-rsdf-kafka-consumer
osd-rsdf-kafka-consumer.application-batch-size-in-seconds = 1
osd-rsdf-kafka-consumer.input-topic = soh.rsdf

osd-systemmessage-kafka-consumer.host = frameworks-osd-systemmessage-kafka-consumer
osd-systemmessage-kafka-consumer.application-id = frameworks-osd-systemmessage-kafka-consumer
osd-systemmessage-kafka-consumer.application-batch-size-in-seconds = 1
osd-systemmessage-kafka-consumer.input-topic = system.system-messages

soh-status-change-kafka-consumer.host = soh-status-change-kafka-consumer
soh-status-change-kafka-consumer.application-id = soh-status-change-kafka-consumer
soh-status-change-kafka-consumer.application-batch-size-in-seconds = 1
soh-status-change-kafka-consumer.input-topic = soh.status-change-event

osd-station-soh-kafka-consumer.host = frameworks-osd-station-soh-kafka-consumer
osd-station-soh-kafka-consumer.application-batch-size-in-seconds = 1
osd-station-soh-kafka-consumer.application-id = frameworks-osd-station-soh-kafka-consumer
osd-station-soh-kafka-consumer.input-topic = soh.station-soh

preloader.c3p0_connection_pool_size = 100

soh-quieted-list-kafka-consumer.host = soh-quieted-list-kafka-consumer
soh-quieted-list-kafka-consumer.application-batch-size-in-seconds = 1
soh-quieted-list-kafka-consumer.application-id = soh-quieted-list-kafka-consumer
soh-quieted-list-kafka-consumer.input-topic = soh.quieted-status-change

capability-soh-rollup-kafka-consumer.host = capability-soh-rollup-kafka-consumer
capability-soh-rollup-kafka-consumer.application-batch-size-in-seconds =1
capability-soh-rollup-kafka-consumer.application-id = capability-soh-rollup-kafka-consumer
capability-soh-rollup-kafka-consumer.input-topic = soh.capability-rollup


# global kafka properties for producers/consumers
kafka-bootstrap-servers = kafka1:9092,kafka2:9092,kafka3:9092
kafka-compression-type = gzip
kafka-key-serializer = org.apache.kafka.common.serialization.StringSerializer
kafka-value-serializer = org.apache.kafka.common.serialization.StringSerializer
kafka-key-deserializer = org.apache.kafka.common.serialization.StringDeserializer
kafka-value-deserializer = org.apache.kafka.common.serialization.StringDeserializer

# session timeout for consumers (default to 10s measured in ms)
kafka-consumer-session-timeout = 10000

# heartbeat interval measured milliseconds
kafka-consumer-heartbeat-interval = 3000

# reactor kafka consumer settings
reactor-kafka-consumer-session-timeout = 60000
reactor-kafka-consumer-max-poll-interval = 2500
reactor-kafka-consumer-max-poll-records = 2000
reactor-kafka-auto-commit = false
reactor-kafka-consumer-heartbeat-interval = 3000

# reactor kafka sender settings
reactor-kafka-sender-transaction-timeout = 30000
reactor-kafka-sender-acks = all
reactor-kafka-sender-delivery-timeout = 120000 

# kafka properties
verification-attempts = 15
streams-close-timeout-ms = 120000
connection-retry-count = 10
retry-backoff-ms = 1000

# gms kafka topics
kafka-rsdf-topic = soh.rsdf
kafka-acquiredchannelsoh-topic = soh.acei
kafka-stationsohinput-topic = soh.extract
kafka-malformed-topic = malformed.frames

# Config for Configuration Consumers
config-cache-expiration = PT24H

# Global config for ConnMan/DataMan
cd11-dataconsumer-baseport = 8100

# Config for cd11 ConnMan control
connman.data-manager-ip-address = da-dataman
connman.experimental-enabled = true
connman.connection-manager-well-known-port = 8041
#data-provider-ip-address will be used for validation in the future but now is just used in log statements
connman.data-provider-ip-address = localhost

#Config for DataMan
dataman.application-id = dataman
dataman.experimental-enabled = true 
dataman.reactor-kafka-key-serializer = org.apache.kafka.common.serialization.Serdes$StringSerde
dataman.reactor-kafka-value-serializer = org.apache.kafka.common.serialization.Serdes$StringSerde

# Config for CD1.1 RSDF Processor
cd11-rsdf-processor.application-id = cd11-rsdf-processor
cd11-rsdf-processor.experimental-enabled = true
cd11-rsdf-processor.reactor-kafka-key-serializer = org.apache.kafka.common.serialization.Serdes$StringSerde
cd11-rsdf-processor.reactor-kafka-value-serializer = org.apache.kafka.common.serialization.Serdes$StringSerde

# Config for waveform QC Control
waveform-qc-control.processing-configuration-root = gms/core/waveformqc/configuration-base/

#Config for beam control
beam-control.processing-configuration-root = gms/core/signalenhancement/beam-control/configuration-base

# Config for event-location-control-service
event-location-control.processing-configuration-root = gms/core/event/location/configuration-base
event-location-control.host = event-location-control-service
event-location-control.port = 8080

# Config for signal-detection-association-control-service
signal-detection-association-control.processing-configuration-root = gms/core/event/association/control/baseconfig/

# Config for interactive analysis config service
interactive-analysis-config-service.processing-configuration-root = gms/core/interactiveanalysis/config/service/configuration-base/

# Config for Event Magnitude Control
event-magnitude-control.processing-configuration-root = gms/core/eventmagnitudecontrol/configuration-base/

# Config for Amplitude Control
amplitude-control.processing-configuration-root = gms/core/amplitudecontrol/configuration-base/

#Config for station soh control
soh-control.processing-configuration-root = gms/config/processing-configuration/soh-control/

# Config for ssam-control application
station-soh-analysis-manager.application-id=ssam-application
station-soh-analysis-manager.quieted_list_input_topic=soh.quieted-list
station-soh-analysis-manager.soh_station_input_topic=soh.station-soh
station-soh-analysis-manager.capability_rollup_input_topic=soh.capability-rollup
station-soh-analysis-manager.materialized_view_output_topic=soh.ui-materialized-view
station-soh-analysis-manager.system_message_output_topic=system.system-messages
station-soh-analysis-manager.status_change_input_topic=soh.ack-station-soh
station-soh-analysis-manager.quieted_status_change_output_topic=soh.quieted-status-change
station-soh-analysis-manager.status_change_output_topic=soh.status-change-event

# Config for the soh-control application
soh-control.sohAppId=soh-application
soh-control.application-id=soh-application
soh-control.sohInputTopic=soh.extract
soh-control.stationSohOutputTopic=soh.station-soh
# do we need to add ui topic?
soh-control.capabilitySohRollupOutputTopic=soh.capability-rollup
# Every 10 minutes, it will produce logging messages with performance stats.
soh-control.monitorLoggingPeriod=PT10M
# Have the control check for updates to the config every 5 seconds.
soh-control.controlConfigUpdateIntervalMs=5000
# 3 mb. The Kafka default is 1 mb.
soh-control.maxRequestSize=1572864
soh-control.fetchMaxBytes=1572864
soh-control.maxAcquiredBytes=10485760

# Config for the acei-merge-processor
acei-merge-processor.application-id=acei-merge-processor-application
acei-merge-processor.input-acei-topic=soh.acei
acei-merge-processor.merge-tolerance-ms=500
acei-merge-processor.benchmark-logging-period-seconds=600
acei-merge-processor.cache-expiration-period-seconds=1200
acei-merge-processor.storage-period-milliseconds=10000
# Setting to 0 causes it to use Runtime.getRuntime().availableProcessors()
acei-merge-processor.processor-thread-count=0
# Max number of ACEIs that can be inserted or removed per DB interaction
max-items-per-db-interaction=256
# Max number of db operations that can be performed in parallel. Deployments on fin
# can't seem to tolerate parallel ops, so leave this at 1 unless the problem on fin
# is addressed.
max-parallel-db-operations=1
min-items-to-perform-db-operations=1028
# Config for filter control
filter-control.processing-configuration-root = gms/core/signalenhancement/waveformfiltering/configuration-base/
filter-control.max-threads = 200

# Config for fk control
fk-control.processing-configuration-root = gms/core/signalenhancement/fkcontrol/configuration-base/

# Config for signal-detector-control
signal-detector-control.processing-configuration-root = gms/core/signaldetection/signaldetectorcontrol/configuration-base/

client-timeout = PT60S

# Temporarily longer timeouts for StationSohControl and SSAMControl to account for station group query
station-soh-analysis-manager.client-timeout = PT4M
soh-control.client-timeout = PT4M

# Config for Processing Configuration Service
processing-cfg.processing-configuration-root = gms/shared/frameworks/processing/configuration/service/configuration-base/
processing-cfg.host = frameworks-configuration-service
processing-cfg.sql_url = jdbc:postgresql://postgresql-gms:5432/gms
processing-cfg.sql_user = gms_config_application
processing-cfg.sql_password = gmsdb:postgres:gms_config_application:good-steel-referees
processing-cfg.c3p0_connection_pool_size = 5

#config for osd ttl worker
osd-ttl-worker.sql_user = gms_soh_ttl_application
osd-ttl-worker.sql_password = gmsdb:postgres:gms_soh_ttl_application:ceilings-originate-eleventh
osd-ttl-worker.c3p0_connection_pool_size = 10

#see http://www.quartz-scheduler.org/documentation/quartz-2.3.0/tutorials/crontrigger.html for cron scheduling details
osd-ttl-worker.AcquiredChannelEnvironmentIssue.timeToLive = PT168H
osd-ttl-worker.AcquiredChannelEnvironmentIssue.cronSchedule = 0 0 * * * ? *
osd-ttl-worker.RawStationDataFrame.timeToLive = PT168H
osd-ttl-worker.RawStationDataFrame.cronSchedule = 0 0 * * * ? *
osd-ttl-worker.StationSoh.timeToLive = PT168H
osd-ttl-worker.StationSoh.cronSchedule = 0 0 * * * ? *

# config for sohLoader app
soh-loader.station_soh_url = seed-data/stationSoh.json
soh-loader.analog_soh_url = seed-data/analog.json
soh-loader.boolean_soh_url = seed-data/boolean.json
soh-loader.time_span = P30D
soh-loader.soh_datatype_to_load = 3
